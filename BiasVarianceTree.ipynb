{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5. (Bias and Variance in Tree-based Bagging Ensemble Methods)\n",
    "\n",
    "In this problem, we'll investigate the bias and the variance of two different estimators: a 2-split tree and a 4-split tree. We'll see (once again) that fitting the data more precisely is not always a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Suppose our dataset $\\mathcal D$ consists of $n=10$ data points drawn randomly from two moons and is balanced. Points are labeled 1 in one moon and 0 in the other moon. We set 0.05 to be the standard deviation of Gaussian noise in the data.\n",
    "\n",
    "Generate a sample dataset using random_state=2, and plot it. (Note: because the data is 2-dimensional, have the x- and y-coordinates on the axes and represent the label with the color blue for 1 and red for 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Fit a decision tree model with 2 splits to $\\mathcal D$. Use random_state= 2. (Hint: for n splits, there are at most n+1 leaf nodes.)\n",
    "\n",
    "Plot this tree in the form of rectangular partitions of the feature space with $\\mathcal D$. Use lines to mark the splits, and shade the regions with the correct label color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Fit a decision tree model with 4 splits to $\\mathcal D$. Use random_state= 2.\n",
    "\n",
    "Plot this tree in the form of rectangular partitions of the feature space with $\\mathcal D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Repeat parts b) and c) to fit models for 1000 different randomly drawn sets $\\mathcal D$ from a newly generated set of 10,000 points using random_state=2. Average the individual 2-split decision trees to get the \"average\" 2-split decision tree model $\\bar{t}(x)$ using the bagging ensemble method learned in class. Plot $\\bar{t}(x)$ for all 10,000 points.\n",
    "\n",
    "Do the same for 4-split decision trees to get $\\bar{f}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "Compute the squared bias of $\\bar{t}(x)$ and $\\bar{f}(x)$. Which model has smaller squared bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Response:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "Compute the variance of $\\bar{t}(x)$ and $\\bar{f}(x)$. Which model has smaller variance? How do you interpret this? Which model has smaller overall error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Response:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g)\n",
    "\n",
    "How do you think your results (for bias, variance, and total error) depend on the number of points used to train each bagged model? Perform an experiment to check. Explain what you observe.\n",
    "\n",
    "For clarity, here is a list of parameters you should be considering:\n",
    "\n",
    "* number of bagged models = 1000\n",
    "* number of points used to train each bagged model = $n \\in \\{5, 10, 15, ..., 30 \\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Response:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h)\n",
    "\n",
    "As we increase the number of models to average in our bagging ensemble method, we should expect our model (which may be quite complex!) to not only fit the training data better but to also generalize better to a test set.\n",
    "\n",
    "Using a test set of size 10000 with noise=0.05 and random_state=3, verify this by computing the average out-of-sample misclassification error over 10 replications for each choice of number of bagged models in the range $R = \\{1,5,10,20,30,40,50,75,100,150\\}$. Use 4-split tree models trained on randomly chosen datasets of size $10$ and plot averaged test error vs number of bagged models.\n",
    "\n",
    "In particular, for each number of bagged models $r\\in R$, build 10 different 4-split tree models with n_estimators = $r$ and max_samples=10, and average the out-of-sample misclassification error over all 10 models. \n",
    "\n",
    "Note that averaging over many replications reduces the inherent variability in the noise of the dataset.\n",
    "\n",
    "For clarity, here is a list of parameters that you should be considering:\n",
    "\n",
    "* number of bagged models = $r \\in R = \\{1,5,10,20,30,40,50,75,100,150\\} $\n",
    "* number of replications = number of bagging classifiers to train for a given $r$ = 10 \n",
    "* number of points used to train each bagged model = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
