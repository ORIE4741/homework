{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:**\n",
    "\n",
    "ORIE 4741 uses peer grading to determine scores on final projects. Each project has an underlying quality; some are good, some less good. Some students are fair graders, and report the project quality as their grade. Some are easy graders, and report a higher grade. Some are harsh graders, and report a lower grade.\n",
    "As a result, some students fear that peer grading is unfair: why should their grade suffer simply through the random chance of a harsh reviewer?\n",
    "\n",
    "An ideal solution might be to have every student grade every project. \n",
    "However, this solution is rarely popular with reviewers.\n",
    "Instead, in this homework problem, we will explore whether we can predict the ratings \n",
    "that all other reviewers *would have* given had they reviewed all projects. \n",
    "We will try this technique both on a synthetic data and on the real peer-review rating scores for ORIE4741 projects in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, let's define our problem. \n",
    "There are $d$ students enrolled in ORIE 4741 who have formed $n$ project groups.\n",
    "Each student is responsible for grading $p$ projects. \n",
    "(In our class, $p$=2.)\n",
    "We'll collect the grades into a grade matrix $A \\in \\mathbb{R}^{n \\times d}$: \n",
    "$A_{ij}$ will represent the grade that student $j$ would assign to project $i$.\n",
    "\n",
    "Of course, we cannot assign each student to grade every project. Instead, we make peer review assignments $\\Omega = \\{(i_1, j_1), \\ldots, \\}$. Here, $(i,j) \\in \\Omega$ if student $j$ is assigned to grade project $i$. \n",
    "\n",
    "Unfortunately, this means that some projects are assigned harder graders than other projects. Our goal is to find a fair way to compute a project's final grade. We consider two methods:\n",
    "\n",
    "1.  *Averaging.* The grade $g_i$ for project $i$ is the average of the grades given by peer reviewers:\n",
    "\n",
    "    $$g^\\text{avg}_i = \\frac 1 p \\sum_{j: (i,j) \\in \\Omega} A_{ij}$$\n",
    "\n",
    "2.  *Matrix completion.* We fit a low rank model to the grade matrix and use it to compute an estimate $\\hat A$ of the grade matrix. We will try a few different losses $\\ell$ and regularizers $r$ to see which work best.\n",
    "<!--    To be more concrete, let's suppose that we find $\\hat A$ by fitting a rank $k$ model. We will use Huber loss, for robustness against outlier grades, and nonnegative regularization, since both student grading toughness and project quality are nonnegative. -->\n",
    "    $$\\text{minimize} \\quad \\sum_{(i,j) \\in \\Omega} \\ell(A_{ij} - (X^T Y)_{ij}) + r(X) + r(Y), $$\n",
    "    where $X \\in \\mathbb{R}^{k \\times n}$ and $Y \\in \\mathbb{R}^{k \\times d}$. \n",
    "    \n",
    "    We will then compute our estimate $\\hat A$ as \n",
    "    $$ \\hat A = X^T Y. $$\n",
    "    In other words, $\\hat A$ is the rank-$k$ matrix that matches the observations best in the sense of the losses $\\ell$ and regularizers $r$ we picked.\n",
    "    \n",
    "    We compute the grade $g_i$ for project $i$ as the average of these estimated grades:\n",
    "\n",
    "    $$g^\\text{mc}_i = \\frac 1 n \\sum_{j=1}^n \\hat A_{ij}$$\n",
    "\n",
    "In this problem, we will consider which of these two grading schemes, averaging or matrix completion, is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LowRankModels, Random, LinearAlgebra, Plots, Statistics, CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Throughout the notebook, we will be using random seeds for reproducibility. Do not remove the corresponding commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first take a look at a demo. We generate a synthetic dataset that contains ratings with integer values 1-5. 1 is lowest and 5 is highest. The rows are aligned by projects; the columns are aligned by reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows are papers, columns are reviewers\n",
    "Random.seed!(1)  # seed random seed, for reproducibility\n",
    "\n",
    "# problem dimensions\n",
    "n,d = 100,200    # n projects, d reviewers\n",
    "\n",
    "# observed entries\n",
    "Ω = findall(rand(n,d) .< .1)  # Ω is a matrix with the same shape as our 10% of entries are observed. \n",
    "\n",
    "# latent parameters \n",
    "θ = rand(n)      # quality of paper \n",
    "a = rand(d)      # coefficient of reviewer\n",
    "b = rand(d)      # offset of reviewer\n",
    "\n",
    "# data matrix\n",
    "A = θ * a' .+ b'\n",
    "\n",
    "a = vec(A)\n",
    "t = [quantile(a,.22), quantile(a,.31), quantile(a,.53), quantile(a,.88)] # thresholds\n",
    "\n",
    "# generate ratings matrix\n",
    "R = ones(Int, n, d)\n",
    "for ti in t\n",
    "    R += A .> ti\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot histogram of scores\n",
    "histogram(vec(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute observed average scores on all projects\n",
    "observed_average_score = []\n",
    "for i=1:n\n",
    "    append!(observed_average_score, mean(R[filter(t -> t[1] == i, Ω)]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute true average scores on all projects\n",
    "true_average_score = mean(R, dims=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take a look at different matrix completion scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit ratings with Quadratic Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first fit a low rank model to this simulated data using quadratic loss and regularizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1)      \n",
    "loss = QuadLoss()    # quadratic loss\n",
    "reg = QuadReg(.0001) # a tiny bit of quadratic regularization\n",
    "k = 2                # we'll add the offset separately below \n",
    "glrm = GLRM(R, loss, reg, reg, k, obs=Ω)  # the GLRM object stores the model, data, and parameter estimate\n",
    "add_offset!(glrm)    # adds an offset to the model\n",
    "\n",
    "# fit low rank model\n",
    "fit!(glrm)                            # alternating minimization\n",
    "Rhat_quad = impute(glrm)                   # imputed values \n",
    "MAE_quad = sum(abs.(vec(Rhat_quad - R)))/(n*d)  # mean absolute error\n",
    "@show MAE_quad;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of our predictions, we define the following terms:\n",
    "\n",
    "1. observed average score: the average of all scores we observe on a single project.\n",
    "2. true average score: the average of all scores on a single project, whether we observed or not. These scores are regarded as true labels.\n",
    "3. predicted score: the average of all predicted scores on a single project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted scores on all projects\n",
    "pred_score_quad = mean(Rhat_quad, dims=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "\n",
    "Plot histograms of\n",
    "1. the difference between the observed average score and the true average score\n",
    "2. the difference between the predicted average score and the true average score\n",
    "\n",
    "Notice that for real data, the true average score is not known, while the observed average score is known and the predicted average score is computable.\n",
    "Which predicts the true average score best? Compare the variance of the errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"YOUR WORK HERE: histograms\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"YOUR WORK HERE: variance comparison\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit with the BvSLoss function\n",
    "\n",
    "Notice that ratings are ordinal, taking values between 1 and 5. \n",
    "Hence it makes more sense to use an ordinal loss. \n",
    "We will use BvSLoss (Bigger-versus-Smaller) here. \n",
    "As we saw in class, using the BvSLoss is equivalent to mapping the matrix through the ordinal embedding\n",
    "    \n",
    "    1 => [0, 0, 0, 0]\n",
    "    2 => [1, 0, 0, 0]\n",
    "    3 => [1, 1, 0, 0]\n",
    "    4 => [1, 1, 1, 0]\n",
    "    5 => [1, 1, 1, 1]\n",
    "    \n",
    " and fitting the resulting matrix with the hinge loss or logistic loss.\n",
    " \n",
    " Fit a model to the data using the BvSLoss and a nonnegative regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a low rank model by selecting appropriate losses, regularizer, and rank k\n",
    "loss = BvSLoss(5, bin_loss=LogisticLoss())    # BvSLoss with 5 levels, using Logistic Loss as the underlying binary loss\n",
    "# you could also ask for nonnegative coefficients:\n",
    "rx = lastentry1(NonNegConstraint())\n",
    "ry = OrdinalReg(NonNegConstraint())\n",
    "# rx = lastentry1(QuadReg(.0001))\n",
    "# ry = OrdinalReg(QuadReg(.0001))\n",
    "k = 2                # we'll add the offset separately below \n",
    "glrm = GLRM(R, loss, rx, ry, k, obs=Ω)  # the GLRM object stores the model, data, and parameter estimate\n",
    "\n",
    "# initialize with random positive numbers\n",
    "glrm.X = rand(size(glrm.X)...)\n",
    "glrm.Y = rand(size(glrm.Y)...)\n",
    "\n",
    "# fit low rank model \n",
    "fit!(glrm)                            # alternating minimization\n",
    "Rhat_bvs = impute(glrm)                   # imputed values \n",
    "MAE_bvs = sum(abs.(vec(Rhat_bvs - R)))/(n*d)  # mean absolute error\n",
    "@show MAE_bvs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_score_bvs = mean(Rhat_bvs, dims=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "\n",
    "Plot a histogram of the error between the predicted score using the BvSLoss and the true average score.\n",
    "\n",
    "Compare the accuracy of the predictions from the model fit with BvSLoss and QuadLoss. Which predicts the ratings better (say, in mean absolute error)? Which predicts the true average score better (say, in mean absolute error)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"YOUR WORK HERE: histograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"YOUR WORK HERE: accuracy comparison by, e.g., mean absolute error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "\n",
    "Fit at least one more model to this dataset. How did you choose which loss or regularizers to use? How does the error of your method compare to the error of the two models we fit above?\n",
    "\n",
    "The ORIE 4741 must choose a method to grade your projects based on observable data. Based on these results on synthetic data, how would you like them to grade you? (Note: you'll have a chance to answer again after you see the real data.)\n",
    "\n",
    "1. average the observed scores \n",
    "2. average the scores imputed by matrix completion with quadratic loss\n",
    "3. average the scores imputed by matrix completion with BvSLoss\n",
    "4. average the scores imputed by matrix completion with my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"YOUR ANSWER HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Fall 2017 ORIE4741 project review data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well matrix completion works on the real data!\n",
    "The rating scores are ordinal and have 6 possible values.\n",
    "Here, we don't have access to the true average score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = CSV.read(\"ratings.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_score = ratings[:, 2]; # rating scores given by staff\n",
    "average_score = ratings[:, 3]; # rating scores from averages of peer review scores\n",
    "R_real = convert(Matrix, ratings[:, 4:end]); # peer review grades\n",
    "Ω_real = findall(.!ismissing.(R_real)); # observed entries\n",
    "n_real, p_real = size(R_real);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's plot the average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(average_score, nbins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit ratings with several losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we choose Huber loss with nonnegative constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a low rank model by selecting appropriate losses, regularizer, and rank k\n",
    "Random.seed!(1)\n",
    "loss = HuberLoss() \n",
    "reg = NonNegConstraint() \n",
    "k = 2                # we'll add the offset separately below \n",
    "glrm = GLRM(R_real, loss, reg, reg, k, obs=Ω_real)\n",
    "add_offset!(glrm)    # adds an offset to the model\n",
    "\n",
    "# fit low rank model \n",
    "fit!(glrm)                            # alternating minimization\n",
    "Rhat_real_huber = impute(glrm)                   # imputed values \n",
    "MAE_real_huber = sum(abs.(vec((Rhat_real_huber - R_real)[Ω_real])))/(n_real*p_real)  # mean absolute error\n",
    "pred_score_quad = mean(Rhat_real_huber, dims=2)  # predicted scores of each project\n",
    "@show MAE_real_huber;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the predictions\n",
    "histogram(pred_score_quad, nbins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the L1 loss performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a low rank model by selecting appropriate losses, regularizer, and rank k\n",
    "Random.seed!(1)\n",
    "loss = L1Loss()    # BvSLoss with 5 levels\n",
    "reg = QuadReg(.0001) # a tiny bit of quadratic regularization\n",
    "k = 2                # we'll add the offset separately below \n",
    "glrm = GLRM(R_real, loss, reg, reg, k, obs=Ω_real)  # the GLRM object stores the model, data, and parameter estimate\n",
    "add_offset!(glrm)    # adds an offset to the model\n",
    "\n",
    "# fit low rank model\n",
    "fit!(glrm)                            # alternating minimization\n",
    "Rhat_real_l1 = impute(glrm)                   # imputed values \n",
    "MAE_real_l1 = sum(abs.(vec((Rhat_real_l1 - R_real)[Ω_real])))/(n_real*p_real)  # mean absolute error\n",
    "@show MAE_real_l1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_score_l1 = mean(Rhat_real_l1, dims=2)\n",
    "histogram(pred_score_l1, nbins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit ratings with BvSLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience in fitting the BvSLoss, we create a copy of the peer grades with missing entries imputed by 0. This won't affect the fit of the GLRM, since we restrict it to look only at the observed entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_real_noNA = copy(R_real);\n",
    "for row in 1:n_real\n",
    "      for col in 1:p_real\n",
    "        if  ismissing(R_real[row, col])\n",
    "            R_real_noNA[row, col] = 0\n",
    "        end\n",
    "      end\n",
    "end\n",
    "R_real_noNA = Int.(R_real_noNA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1)\n",
    "loss = BvSLoss(6, bin_loss=LogisticLoss())    # BvSLoss with 6 levels, using Logistic Loss as the underlying binary loss\n",
    "# could also ask for nonnegative coefficients:\n",
    "rx = lastentry1(QuadReg(.0001))\n",
    "ry = OrdinalReg(QuadReg(.0001))\n",
    "\n",
    "# rx = lastentry1(NonNegConstraint())\n",
    "# ry = OrdinalReg(NonNegConstraint())\n",
    "k = 2                # we'll add the offset separately below \n",
    "glrm = GLRM(R_real_noNA, loss, rx, ry, k, obs=Ω_real)  # the GLRM object stores the model, data, and parameter estimate\n",
    "\n",
    "# initialize with random positive numbers\n",
    "glrm.X = rand(size(glrm.X)...)\n",
    "glrm.Y = rand(size(glrm.Y)...)\n",
    "\n",
    "# fit low rank model \n",
    "fit!(glrm)                            # alternating minimization\n",
    "Rhat_real_bvs = impute(glrm)                   # imputed values \n",
    "MAE_real_bvs = sum(abs.(vec((Rhat_real_bvs - R_real)[Ω_real])))/(n_real*p_real)  # mean absolute error\n",
    "@show MAE_real_bvs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_score_bvs = mean(Rhat_real_bvs .- 0, dims=2);\n",
    "histogram(pred_score_bvs, nbins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "\n",
    "Fit at least one more model to this dataset. How did you choose which loss or regularizers to use? How does the error of your method compare to the error of the two models we fit above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"YOUR WORK HERE: more model fitting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Comment on the performance of all of these models on real data. \n",
    "Your answer should discuss how the prediction errors are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"YOUR ANSWER HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ORIE 4741 must choose a method to grade your projects based on observable data. How would you like them to grade you?\n",
    "1. average the observed scores \n",
    "2. average the scores imputed by matrix completion with quadratic loss\n",
    "3. average the scores imputed by matrix completion with BvSLoss\n",
    "4. average the scores imputed by matrix completion with my model\n",
    "\n",
    "**Question**: Do you prefer simple grade averaging or matrix completion? If you prefer matrix completion, state what loss and regularizer you'd like the course staff to use (the polling result may affect your staff's choice!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"YOUR ANSWER HERE\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
